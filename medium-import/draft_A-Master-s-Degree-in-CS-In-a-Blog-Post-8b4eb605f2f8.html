<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>A Master’s Degree in CS In a Blog Post</title>
    <style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html,
      body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field="subtitle"],
      section[data-field="description"] {
        display: none;
      }
    </style>
  </head>
  <body>
    <article class="h-entry">
      <header>
        <h1 class="p-name">A Master’s Degree in CS In a Blog Post</h1>
      </header>
      <section data-field="subtitle" class="p-summary">PROLOGUE</section>
      <section data-field="body" class="e-content">
        <section
          name="da5c"
          class="section section--body section--first section--last"
        >
          <div class="section-divider"><hr class="section-divider" /></div>
          <div class="section-content">
            <div class="section-inner sectionLayout--insetColumn">
              <h3
                name="92de"
                id="92de"
                class="graf graf--h3 graf--leading graf--title"
              >
                A Master’s Degree in CS In a Blog Post
              </h3>
              <p
                name="5b76"
                id="5b76"
                class="graf graf--p graf--empty graf-after--h3"
              >
                <br />
              </p>
              <h3 name="2bdc" id="2bdc" class="graf graf--h3 graf-after--p">
                PROLOGUE
              </h3>
              <p name="604a" id="604a" class="graf graf--p graf-after--h3">
                My master’s degree has had a significant contribution to
                understanding the fundamentals of CS and Artificial
                “Intelligence”(who is “intelligent”?). To give some background,
                I was building software for 3.5 years at an Adtech startup in
                India before I took up a master’s degree. My main intent of
                doing a master’s degree was to clear my fundamentals and
                understand this “AI” thing that all the cool kids on the block
                were talking about.
              </p>
              <p name="e6f7" id="e6f7" class="graf graf--p graf-after--p">
                The idea of this blog post is to combine the experience of the
                last 6 years to synthesize some learnings and some predictions.
                All of the learnings and predictions and are aligned towards
                building software/”ML” and technology. The learnings and
                predictions have arisen from my insights of reading CS research
                over the past year and collating it with trends I have seen in
                the industry/open-source software etc.
              </p>
              <h3 name="2c3f" id="2c3f" class="graf graf--h3 graf-after--p">
                Introduction
              </h3>
              <p name="f90a" id="f90a" class="graf graf--p graf-after--h3">
                The ability to write code in the 21st century is a legit
                superpower. Many would read this and roll their eyes. But if you
                think about it, It does kinda make sense. Programming enables
                you to create what
                <a
                  href="https://www.youtube.com/watch?v=eoLAIDGM4zI"
                  data-href="https://www.youtube.com/watch?v=eoLAIDGM4zI"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener noreferrer noopener"
                  target="_blank"
                  >Naval Ravikant calls “Infinite Leverage”</a
                >. It&#39;s that one skill through which the impact of one’s
                work can have extremely asymmetric returns. But in the earlier
                days, there were boundaries to what one can and couldn’t do with
                software. We are slowly approaching a time where that
                distinction is getting blurrier. Meaning the “superpower” can
                accommodate more “abilities” and “unlocks” if newer knowledge is
                acquired. My choice to get a master’s degree was purely to open
                a few of these “unlocks”. In the process of acquiring these new
                skills, I gained a few key learnings that made me ponder up the
                implications of what I learned. The next sections will discuss
                exactly these things: The learnings and predictions based on
                them. Everything thing written here should be taken with a grain
                of salt. Even though I am really optimistic about “Machine
                learning” and “Neural networks”, I will mention that for many
                problems neural networks are just brittle/overkill, and there’s
                currently just a gold-rush for publishing in the field so by the
                time you may be reading this, some ideas here may be outdated.
                And never forget:
                <a
                  href="https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization"
                  data-href="https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >There is no such thing as a free lunch</a
                >
              </p>
              <h3 name="e205" id="e205" class="graf graf--h3 graf-after--p">
                Learnings
              </h3>
              <p name="be4f" id="be4f" class="graf graf--p graf-after--h3">
                All the “AI” that creates awe has been created in the last
                decade(Even though conceptualized
                <strong class="markup--strong markup--p-strong">much</strong>
                earlier). People have been astonished when I tell them that a
                decade ago a huge number of benchmarks were not even there! It
                all started in 2012
                <a
                  href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html"
                  data-href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener noreferrer noopener"
                  target="_blank"
                  >when a bunch of researchers from Toronto took a decades-old
                  idea and made it work for classifying images</a
                >. This event “broke computer vision”. What I mean by “Breaking
                computer vision” is that this system was several folds better
                than existing technology!. It was “State Of the Art”(SOTA).
                (Nowadays we have a new SOTA every few weeks which makes me
                ponder on the “State” of SOTA). This is what started the entire
                gold rush and the fundamental technology(Neural networks) which
                helped create this system started getting applied to other more
                harder and challenging problems.
              </p>
              <p name="88dd" id="88dd" class="graf graf--p graf-after--p">
                This historical context can be seen best from the below timeline
                where interesting discoveries/inventions like
                <a
                  href="https://arxiv.org/abs/1406.2661"
                  data-href="https://arxiv.org/abs/1406.2661"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener noreferrer noopener"
                  target="_blank"
                  >GANS</a
                >(Converting images of zebras to horses and horses to zebras)
                came in 2014 and it all further escalated over the next few
                years to gameplaying(<a
                  href="https://arxiv.org/abs/1712.01815"
                  data-href="https://arxiv.org/abs/1712.01815"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener noreferrer noopener"
                  target="_blank"
                  >GO</a
                >
                and
                <a
                  href="https://arxiv.org/abs/1912.06680"
                  data-href="https://arxiv.org/abs/1912.06680"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener noreferrer noopener"
                  target="_blank"
                  >DOTA 2</a
                >),
                <a
                  href="https://arxiv.org/abs/1810.04805"
                  data-href="https://arxiv.org/abs/1810.04805"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener noreferrer noopener"
                  target="_blank"
                  >language translation</a
                >,
                <a
                  href="https://openai.com/blog/musenet/"
                  data-href="https://openai.com/blog/musenet/"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener noreferrer noopener"
                  target="_blank"
                  >music generation</a
                >
                and
                <a
                  href="https://arxiv.org/abs/2005.14165"
                  data-href="https://arxiv.org/abs/2005.14165"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener noreferrer noopener"
                  target="_blank"
                  >even writing authentic-looking essays</a
                >. It&#39;s now even helping create
                <a
                  href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology"
                  data-href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener noreferrer noopener"
                  target="_blank"
                  >“good enough” predictions for folding proteins</a
                >. This timeline tries to note down some key events which are
                game-changing “Firsts”.
              </p>
              <figure
                name="18ec"
                id="18ec"
                class="graf graf--figure graf--layoutOutsetLeft graf-after--p"
              >
                <img
                  class="graf-image"
                  data-image-id="1*DqFN2bhxgzc7-mFXlWzHVw.jpeg"
                  data-width="960"
                  data-height="540"
                  src="https://cdn-images-1.medium.com/max/600/1*DqFN2bhxgzc7-mFXlWzHVw.jpeg"
                />
              </figure>
              <figure
                name="c6ac"
                id="c6ac"
                class="graf graf--figure graf--layoutOutsetLeft graf-after--figure"
              >
                <img
                  class="graf-image"
                  data-image-id="1*7rk-ERAt2rRvnemfooLwgA.jpeg"
                  data-width="960"
                  data-height="540"
                  src="https://cdn-images-1.medium.com/max/600/1*7rk-ERAt2rRvnemfooLwgA.jpeg"
                />
              </figure>
              <p name="603c" id="603c" class="graf graf--p graf-after--figure">
                The key insight from this historical context is that the
                wide-scale usage of this tech is NEW!. It&#39;s not as old as
                the C compiler or the Linux operating system. New insights and
                ideas are emerging as the technology is democratized via
                opensource packages!.
              </p>
              <p name="3daf" id="3daf" class="graf graf--p graf-after--p">
                My master’s degree gave me a lot of time to read research around
                this topic and synthesize a few “meta” insights I learned about
                this field
              </p>
              <p
                name="8224"
                id="8224"
                class="graf graf--p graf--empty graf-after--p"
              >
                <br />
              </p>
              <p
                name="38bc"
                id="38bc"
                class="graf graf--p graf--empty graf-after--p"
              >
                <br />
              </p>
              <p
                name="5515"
                id="5515"
                class="graf graf--p graf--empty graf-after--p"
              >
                <br />
              </p>
              <h3 name="a1c4" id="a1c4" class="graf graf--h3 graf-after--p">
                Learning 1: Neural Networks Are Glorious Function Approximators
              </h3>
              <p name="8824" id="8824" class="graf graf--p graf-after--h3">
                Any software engineer with basic knowledge of programming is
                aware of functions. We write functions in software all the time.
                We use them all the time. A function is a means through which we
                map some input to some output. Generally, we write functions to
                get database records or do REST API calls or sort arrays etc.
                This is the bread and butter of any software dev. The cool thing
                with neural networks is that they are
                <em class="markup--em markup--p-em">approximate functions</em>
                that can translate an input to an
                <em class="markup--em markup--p-em">approximate</em> output.
                This is a profound insight as lots of problems can be framed as
                functions and objects !.
              </p>
              <p name="3a89" id="3a89" class="graf graf--p graf-after--p">
                Neural networks provide a way to create functions that can have
                complex input data like free text or images or videos and use
                them to yield an output which could, in turn, be complex data
                like free text and images or just “classification labels” or a
                “real number value/s “.
              </p>
              <h4 name="13de" id="13de" class="graf graf--h4 graf-after--p">
                Neural Networks through the lens of abstractions
              </h4>
              <p name="76e8" id="76e8" class="graf graf--p graf-after--h4">
                Abstractions and functions are the bread and butter of S/W
                engineers. Wikipedia has a Formal(Very Dull) Definition of
                Abstraction:
              </p>
              <blockquote
                name="ff88"
                id="ff88"
                class="graf graf--blockquote graf-after--p"
              >
                In software engineering and computer science, abstraction is: 
              </blockquote>
              <blockquote
                name="dba7"
                id="dba7"
                class="graf graf--blockquote graf-after--blockquote"
              >
                — the process of removing physical, spatial, or temporal details
                or attributes in the study of objects or systems to focus
                attention on details of greater importance; it is similar in
                nature to the process of generalization; 
              </blockquote>
              <blockquote
                name="d282"
                id="d282"
                class="graf graf--blockquote graf-after--blockquote"
              >
                — the creation of abstract concept-objects by mirroring common
                features or attributes of various non-abstract objects or
                systems of study — the result of the process of abstraction.
              </blockquote>
              <p
                name="fa09"
                id="fa09"
                class="graf graf--p graf-after--blockquote"
              >
                This is a very dirty explanation. Let me simplify it.
                “Abstraction” as a word can be used in multiple contexts but
                when used in the context of creating software,
                <strong class="markup--strong markup--p-strong"
                  ><em class="markup--em markup--p-em"
                    >it means creating/conceptualizing a generalizable
                    structured hierarchy of components/concepts/object/things,
                    etc. that address some specific problem/query.</em
                  ></strong
                >
              </p>
              <p name="1f56" id="1f56" class="graf graf--p graf-after--p">
                Creating abstractions is the process of creating order from
                chaos. Your laptop can be seen as an abstraction to connect you
                to the internet. The accelerate and brake paddles in your car
                can be seen as an abstraction for moving the car. The money and
                the currency we hold are an abstraction for the value we can
                attain for goods in society.
              </p>
              <p name="d403" id="d403" class="graf graf--p graf-after--p">
                Abstractions help hide out complex machinery/functionality. They
                create a means through which we don’t need to “fully”
                understanding the inner workings of the system to use the
                system. Simplifying this:
              </p>
              <ul class="postList">
                <li name="76b4" id="76b4" class="graf graf--li graf-after--p">
                  Your Laptop is an abstraction that can help you surf the
                  internet. But the laptop can have many components like RAM,
                  hard-disk, etc. which are structurally compiled together to.
                  Your knowledge of the exact functioning of the underlying
                  components is not necessary for you to “Google something”.
                  Meaning In order for you to leverage the abstraction(your
                  laptop) you don’t need to have the complete underlying
                  knowledge of what is happening exactly in the underlying
                  abstraction.
                </li>
              </ul>
              <p name="7cd3" id="7cd3" class="graf graf--p graf-after--li">
                <strong class="markup--strong markup--p-strong"
                  ><em class="markup--em markup--p-em"
                    >All complex problems can be broken down into simpler
                    subproblems that pieced together to create a whole solution.
                    Meaning all complex problems can have different abstractions
                    for subproblems which together help create the outcome. The
                    same paradigm can be applied to neural networks. Just the
                    way we compose multiple functions working together inside
                    one function to get an output; Neural networks can be
                    decomposed to solve small sub-problems.</em
                  > </strong
                >A really awesome example of this is how
                <a
                  href="https://www.youtube.com/watch?v=hx7BXih7zx8"
                  data-href="https://www.youtube.com/watch?v=hx7BXih7zx8"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener noreferrer noopener"
                  target="_blank"
                  >Tesla Autopilot trains its Neural networks</a
                >
                for self-driving. Even when they are solving a complex problem,
                they design efficient abstractions to break the problem into
                manageable chunks such that meaningful solutions can be created
                by approximating each of the subproblems individually!.
              </p>
              <h4 name="bd0b" id="bd0b" class="graf graf--h4 graf-after--p">
                Test Cases Matter(A Lot)
              </h4>
              <p name="2bf5" id="2bf5" class="graf graf--p graf-after--h4">
                Any good Software engineer worth their salt would have written
                test cases at some point. Sometimes, you may be able to get away
                with only functional testing, without needing more metrics. For
                Neural networks, Test cases and reporting around test cases is
                paramount.
                <strong class="markup--strong markup--p-strong"
                  ><em class="markup--em markup--p-em"
                    >Your test cases should NOT just include running your
                    network on the “test set”, It should include more granular
                    metrics on what failed, What are the patterns to the
                    failure, etc</em
                  ></strong
                >. Any machine learning project is prone to fail if one doesn’t
                have visibility on why it failed!.
                <strong class="markup--strong markup--p-strong"
                  >Not having enough reporting actually has a higher risk of
                  failure because you are missing what is missing.</strong
                >
                Even looking at a ranked list of all failures on the test set
                <em class="markup--em markup--p-em"
                  >is better than just seeing</em
                >
                a high-level metric of X% accuracy/F1/Precision yada yada.
                Ideally, any ML project should focus on test-case building first
                before building the actual “model”.
              </p>
              <h3 name="7541" id="7541" class="graf graf--h3 graf-after--p">
                Learning 2: Techniques For “Efficiently” Managing/Versioning
                Large Neural Networks are “Scalable” given $$
              </h3>
              <p name="62a0" id="62a0" class="graf graf--p graf-after--h3">
                Git in my opinion is one of the most important inventions of the
                software industry. It created a way to make multiple people work
                on the same problem without each of them messing up a good solid
                working idea. It made errors reproducible, It made understanding
                the behavior of the written code more interpretable. With the
                entire concept of branches and Forks in Git, we are able to
                “deterministically” work upon solving problems because we know
                the
                <em class="markup--em markup--p-em"
                  >outcome of an isolated computation won’t change*(given test
                  cases yada yada). </em
                >Approximated functions are tricker this way!.
              </p>
              <p name="640b" id="640b" class="graf graf--p graf-after--p">
                Neural networks can be seen as these huge matrices that
                represent a function taking input and giving an output. The
                process of creating these functions(also called “Training”)
                requires the input and output data of that function in order to
                approximate the function. As the function is “an
                <em class="markup--em markup--p-em">approximate”</em>, there
                will always be a requirement to create a more “accurate” output.
                So versioning code is not enough!. It needs the versioning of
                data, code, and artifacts generated during training/testing.
              </p>
              <p name="1f0b" id="1f0b" class="graf graf--p graf-after--p">
                A lot of recent developments in the current Neural network/deep
                learning field can be attributed to a few paradigm shifts which
                bring their own sets of problems when thinking about versioning
                and scaling:
              </p>
              <ul class="postList">
                <li name="4da3" id="4da3" class="graf graf--li graf-after--p">
                  <a
                    href="https://ruder.io/transfer-learning/"
                    data-href="https://ruder.io/transfer-learning/"
                    class="markup--anchor markup--li-anchor"
                    rel="noopener"
                    target="_blank"
                    >Transfer Learning</a
                  >: <em class="markup--em markup--li-em">One Liner</em>:
                  transfer the learnings of a neural network from one task into
                  another task.
                </li>
                <li name="4178" id="4178" class="graf graf--li graf-after--li">
                  <a
                    href="https://www.youtube.com/watch?v=8TTK-Dd0H9U"
                    data-href="https://www.youtube.com/watch?v=8TTK-Dd0H9U"
                    class="markup--anchor markup--li-anchor"
                    rel="noopener"
                    target="_blank"
                    >Self-supervised/Unsupervised Learning</a
                  >: <em class="markup--em markup--li-em">One Liner</em>: make
                  neural networks learn “on their own” using lots of data.
                </li>
                <li name="899f" id="899f" class="graf graf--li graf-after--li">
                  <a
                    href="https://arxiv.org/abs/1706.03762https://arxiv.org/abs/1706.03762"
                    data-href="https://arxiv.org/abs/1706.03762https://arxiv.org/abs/1706.03762"
                    class="markup--anchor markup--li-anchor"
                    rel="noopener"
                    target="_blank"
                    >Transformers</a
                  >: <em class="markup--em markup--li-em">One Liner: </em>neural
                  networks that can recognize patterns from sequential data.
                  Apparently, just sticking them to
                  <a
                    href="https://arxiv.org/abs/2005.14165"
                    data-href="https://arxiv.org/abs/2005.14165"
                    class="markup--anchor markup--li-anchor"
                    rel="noopener"
                    target="_blank"
                    >many</a
                  >
                  <a
                    href="https://openai.com/blog/clip/"
                    data-href="https://openai.com/blog/clip/"
                    class="markup--anchor markup--li-anchor"
                    rel="noopener"
                    target="_blank"
                    >problems</a
                  >
                  <a
                    href="http://arxiv.org/abs/2102.07074v1"
                    data-href="http://arxiv.org/abs/2102.07074v1"
                    class="markup--anchor markup--li-anchor"
                    rel="noopener"
                    target="_blank"
                    ><em class="markup--em markup--li-em">seems</em></a
                  >
                  to be
                  <a
                    href="https://openai.com/blog/dall-e/"
                    data-href="https://openai.com/blog/dall-e/"
                    class="markup--anchor markup--li-anchor"
                    rel="noopener"
                    target="_blank"
                    >giving</a
                  >
                  <a
                    href="https://arxiv.org/abs/2010.11929"
                    data-href="https://arxiv.org/abs/2010.11929"
                    class="markup--anchor markup--li-anchor"
                    rel="noopener"
                    target="_blank"
                    >pretty</a
                  >
                  <a
                    href="https://arxiv.org/abs/2006.14806"
                    data-href="https://arxiv.org/abs/2006.14806"
                    class="markup--anchor markup--li-anchor"
                    rel="noopener"
                    target="_blank"
                    >good</a
                  >
                  results. 
                </li>
                <li name="671d" id="671d" class="graf graf--li graf-after--li">
                  <a
                    href="https://arxiv.org/abs/2005.14165"
                    data-href="https://arxiv.org/abs/2005.14165"
                    class="markup--anchor markup--li-anchor"
                    rel="noopener"
                    target="_blank"
                    >Making Neural Networks Bigger</a
                  >: <em class="markup--em markup--li-em">One Liner: </em>Just
                  add more layers bro!
                </li>
              </ul>
              <p name="8e95" id="8e95" class="graf graf--p graf-after--li">
                These paradigm shifts are really important as they have helped
                create huge performance gains. The below chart(Figure 1) is one
                of the best indicators of the usefulness of Transfer Learning
                and Self supervised learning. In this chart, the authors make a
                neural network learn to write text(“pretraining”/ ”unsupervised
                training”), and then “Finetune this learned model” to write
                python code. This “finetuned” model performs way better than a
                model trained from scratch in a low data regime. Meaning with
                small amounts of data for a sub-task, the “Finetuned” model can
                perform better than a model learned from scratch!.  
              </p>
              <figure
                name="264c"
                id="264c"
                class="graf graf--figure graf-after--p"
              >
                <img
                  class="graf-image"
                  data-image-id="1*j2QPo5vTciY8cC0HomLDnw.png"
                  data-width="1238"
                  data-height="836"
                  src="https://cdn-images-1.medium.com/max/800/1*j2QPo5vTciY8cC0HomLDnw.png"
                />
                <figcaption class="imageCaption">
                  The chart shows that transfer learning techniques can enable
                  us to learn a new task with much lesser data than the data
                  needed to train the new task from scratch. Source:
                  <a
                    href="https://arxiv.org/abs/2102.01293"
                    data-href="https://arxiv.org/abs/2102.01293"
                    class="markup--anchor markup--figure-anchor"
                    rel="noopener"
                    target="_blank"
                    >https://arxiv.org/abs/2102.01293</a
                  >
                </figcaption>
              </figure>
              <p name="e67d" id="e67d" class="graf graf--p graf-after--figure">
                It is also seen that as we make bigger neural networks, we have
                the capability to learn new tasks with very little data. The
                below chart shows that when we grow the size of the network, we
                can adapt to a different task with very little data and even
                outperform the model of the same size which is trained from
                scratch for that particular task.
              </p>
              <figure
                name="d1f3"
                id="d1f3"
                class="graf graf--figure graf-after--p"
              >
                <img
                  class="graf-image"
                  data-image-id="1*N0A4pDgRK-gpzwPRjDvtxQ.png"
                  data-width="1486"
                  data-height="862"
                  src="https://cdn-images-1.medium.com/max/800/1*N0A4pDgRK-gpzwPRjDvtxQ.png"
                />
                <figcaption class="imageCaption">
                  Bigger Models “pretrained” on lots of data can adapt to new
                  tasks even better than the same models being trained from
                  scratch. Source:
                  <a
                    href="https://arxiv.org/abs/2102.01293"
                    data-href="https://arxiv.org/abs/2102.01293"
                    class="markup--anchor markup--figure-anchor"
                    rel="noopener noreferrer noopener"
                    target="_blank"
                    >https://arxiv.org/abs/2102.01293</a
                  >
                </figcaption>
              </figure>
              <p name="464c" id="464c" class="graf graf--p graf-after--figure">
                All of these seem very shiny and fancy in research papers but
                when considering them for production, it can create a variety of
                problems that can be costly at scale. Even training costs lots
                of money as the Neural Networks grow larger(It also costs a lot
                of money as NVIDIA is a monopoly).
                <a
                  href="https://bdtechtalks.com/2020/08/17/openai-gpt-3-commercial-ai/#:~:text=According%20to%20one%20estimate%2C%20training,increase%20the%20cost%20several%2Dfold."
                  data-href="https://bdtechtalks.com/2020/08/17/openai-gpt-3-commercial-ai/#:~:text=According%20to%20one%20estimate%2C%20training,increase%20the%20cost%20several%2Dfold."
                  class="markup--anchor markup--p-anchor"
                  rel="noopener noreferrer noopener"
                  target="_blank"
                  >Training GPT-3 was estimated to be around 4.6M$</a
                >. There is proprietary tech OpenAI is building for training
                these really large networks in a distributed way over a
                Kubernetes cluster. But adding layers of versioning, data
                management over such distributed training creates complexity for
                which there is
                <strong class="markup--strong markup--p-strong">no</strong>
                “Git-like” solution
                <strong class="markup--strong markup--p-strong">or </strong>well
                laid out best practices
                <strong class="markup--strong markup--p-strong">or</strong>
                off-the-shelf opensource-solutions.
              </p>
              <p name="dccd" id="dccd" class="graf graf--p graf-after--p">
                There are really cool tools that do make life easier like
                <a
                  href="https://github.com/Netflix/metaflow"
                  data-href="https://github.com/Netflix/metaflow"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener noreferrer noopener"
                  target="_blank"
                  >Metaflow</a
                >,
                <a
                  href="https://github.com/flyteorg/flyte"
                  data-href="https://github.com/flyteorg/flyte"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener noreferrer noopener"
                  target="_blank"
                  >Flyte</a
                >, etc. But having reproducibility when the Neural Net
                <em class="markup--em markup--p-em">is</em>
                <em class="markup--em markup--p-em"
                  >the main software containing sub-module neural networks all
                  being optimized by multiple people</em
                >
                is still a not-clearly-solved problem!. An instance can be seen
                in
                <a
                  href="https://youtu.be/IHH47nZ7FZU?t=1328"
                  data-href="https://youtu.be/IHH47nZ7FZU?t=1328"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >Andrej Karapath’s talk where he mentions</a
                >
                creating really good “Models”/”Neural Nets” which cannot be
                reproduced when they worked as a team at Tesla. Multiple people
                working on the same network and making so many asynchronous
                modifications that the final model is really good but completely
                unreproducible. 
              </p>
              <h3 name="d8e8" id="d8e8" class="graf graf--h3 graf-after--p">
                Learning 3: All Good Technology Follows the Lindy Effect
              </h3>
              <figure
                name="6190"
                id="6190"
                class="graf graf--figure graf-after--h3"
              >
                <img
                  class="graf-image"
                  data-image-id="1*Aent2GhoCbCC483iAViDcQ.png"
                  data-width="1024"
                  data-height="683"
                  src="https://cdn-images-1.medium.com/max/800/1*Aent2GhoCbCC483iAViDcQ.png"
                />
                <figcaption class="imageCaption">
                  Source:
                  <a
                    href="https://atlasgeographica.com/lindy-effect-explained/"
                    data-href="https://atlasgeographica.com/lindy-effect-explained/"
                    class="markup--anchor markup--figure-anchor"
                    rel="noopener noreferrer noopener"
                    target="_blank"
                    >https://atlasgeographica.com/lindy-effect-explained/</a
                  >
                </figcaption>
              </figure>
              <blockquote
                name="ca03"
                id="ca03"
                class="graf graf--blockquote graf-after--figure"
              >
                The Lindy effect is a theorized phenomenon by which the future
                life expectancy of some non-perishable things like a technology
                or an idea is proportional to their current age, so that every
                additional period of survival implies a longer remaining life
                expectancy.<br />- Wikipedia
              </blockquote>
              <p
                name="3f81"
                id="3f81"
                class="graf graf--p graf-after--blockquote"
              >
                I first heard about the Lindy Effect
                <a
                  href="https://youtu.be/Da0aXfshlxM?list=TLPQMjcwMjIwMjEIMNsj9bfQpg&amp;t=3217"
                  data-href="https://youtu.be/Da0aXfshlxM?list=TLPQMjcwMjIwMjEIMNsj9bfQpg&amp;t=3217"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >from Nassim tableb</a
                >. The lindy effect basically means that technology that is new
                will be replaced by better tech, but the tech which survived the
                test of time will be harder to replace.
                <strong class="markup--strong markup--p-strong"
                  >A byproduct of a lot of technologies which are “Lindy” is
                  that they get abstracted!</strong
                >. I know more engineers who are high-level programming language
                engineers than those who write and debug assembly code!.
                Assembly and C code are old(relatively), but most of the time
                they are abstracted away while being used ubiquitously !. Any
                piece of technology which gets used enough may get “abstracted”
                away in some way so more can be done using it. Let me give a few
                good examples:
              </p>
              <ol class="postList">
                <li name="67cf" id="67cf" class="graf graf--li graf-after--p">
                  Javascript seems Lindy. Front-end frameworks like Angular,
                  Vue, React, etc. are not. Why? Because JS is still standing
                  strong but frameworks keep coming and going every few years.
                  Back in early 2015/2016, AngularJS was all the rage. Now every
                  other kid on the block has to React on their resume. What the
                  frameworks did, is they abstracted the core “Lindy” technology
                  which has stood the test of time.
                </li>
                <li name="c22e" id="c22e" class="graf graf--li graf-after--li">
                  Deep Learning/Neural Networks/Differential Programming seems
                  “Lindy”. The fundamental building blocks are being used
                  ubiquitously. The way we use them over time may change based
                  on new methods discovered via scientific research but the
                  building blocks themselves won’t change(What will we replace
                  Linear Algebra with?).
                </li>
                <li name="4bff" id="4bff" class="graf graf--li graf-after--li">
                  Relational Databases are “Lindy”. This one is just obvious. A
                  huge volume of data every day is stored in relational
                  databases via technologies like Postgres, MariaDB, SQL. These
                  technologies have proven battle-tested functioning for a large
                  period of time. Such tech like relational DB&#39;s can be
                  generally abstracted by new tools that offer ways to manage
                  and scale the DB’s. Eg: AWS managed services.  
                </li>
              </ol>
              <p name="06c0" id="06c0" class="graf graf--p graf-after--li">
                Today we have people building these “Novel” models to do
                specialized things. The recent advances with GPT-3 show how the
                model can
                <a
                  href="https://twitter.com/sharifshameem/status/1282676454690451457?s=20"
                  data-href="https://twitter.com/sharifshameem/status/1282676454690451457?s=20"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >even write React Code</a
                >
                from a plain text description of a UI(Somewhat.). A lot of such
                advances over the past decade only point to the fact that
                “Deep-learning” like technologies are there to stay for at least
                a decade more. And If the current trend towards using deep
                learning for large amounts of problems continues then over a
                period of time there would be efficient enough “General Purpose”
                abstractions for a variety of problems.
              </p>
              <h3 name="e39c" id="e39c" class="graf graf--h3 graf-after--p">
                Predictions
              </h3>
              <p name="85ea" id="85ea" class="graf graf--p graf-after--h3">
                The goal of my master’s degree was to understand the breadth of
                CS instead of going into depth of one field. I even
                <a
                  href="https://sci-genie.com/"
                  data-href="https://sci-genie.com/"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener noreferrer noopener"
                  target="_blank"
                  >built a search engine</a
                >
                to help me keep track of new research and understand patterns
                and trends in publishing to keep an eye on where tech is going.
                The predictions mentioned here can be fully wrong but I am
                optimistic based on the trends I am observing in the research
                community and in the ability of the tech industry to create
                faster and more efficient processors.
              </p>
              <h3 name="e867" id="e867" class="graf graf--h3 graf-after--p">
                Prediction 1: Neural Networks will help augment our “User
                Interfaces”
              </h3>
              <p name="592e" id="592e" class="graf graf--p graf-after--h3">
                Big-Tech(FAAMG) is Doubling Down On Bigger Neural Models. Google
                already uses
                <a
                  href="https://arxiv.org/abs/1810.04805"
                  data-href="https://arxiv.org/abs/1810.04805"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >BERT</a
                >(A Fancy Neural Network) for
                <a
                  href="https://blog.google/products/search/search-language-understanding-bert/"
                  data-href="https://blog.google/products/search/search-language-understanding-bert/"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >its searches</a
                >(I liked it before that though).
                <a
                  href="https://blogs.microsoft.com/blog/2020/09/22/microsoft-teams-up-with-openai-to-exclusively-license-gpt-3-language-model/"
                  data-href="https://blogs.microsoft.com/blog/2020/09/22/microsoft-teams-up-with-openai-to-exclusively-license-gpt-3-language-model/"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >Microsoft Licensed GPT-3</a
                >.
                <a
                  href="https://www.slideshare.net/AmazonWebServices/alexa-ask-jarvis-to-create-a-serverless-app-for-me-srv315-aws-reinvent-2018https://www.slideshare.net/AmazonWebServices/alexa-ask-jarvis-to-create-a-serverless-app-for-me-srv315-aws-reinvent-2018"
                  data-href="https://www.slideshare.net/AmazonWebServices/alexa-ask-jarvis-to-create-a-serverless-app-for-me-srv315-aws-reinvent-2018https://www.slideshare.net/AmazonWebServices/alexa-ask-jarvis-to-create-a-serverless-app-for-me-srv315-aws-reinvent-2018"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >AWS wants to build Jarvis</a
                >.
                <a
                  href="https://machinelearning.apple.com/research/face-detection"
                  data-href="https://machinelearning.apple.com/research/face-detection"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >Apple is using ML to detect your face</a
                >
                and there are so many more solutions/applications these
                companies aiming for. They are doubling down on this technology.
                <a
                  href="https://arxiv.org/abs/2101.03961"
                  data-href="https://arxiv.org/abs/2101.03961"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >A recent paper from Google uses even bigger models</a
                >
                and proposes methods for building them. There have been
                breakthroughs in
                <a
                  href="https://arxiv.org/abs/2102.01645"
                  data-href="https://arxiv.org/abs/2102.01645"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >neural networks which can describe images</a
                >
                and
                <a
                  href="https://openai.com/blog/dall-e/"
                  data-href="https://openai.com/blog/dall-e/"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >generate images from text descriptions</a
                >. There have also been advances in research where
                <a
                  href="https://arxiv.org/pdf/2004.03805"
                  data-href="https://arxiv.org/pdf/2004.03805"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >“rendering” is done directly via “neural networks”</a
                >.
              </p>
              <p name="8df9" id="8df9" class="graf graf--p graf-after--p">
                Roughly half the papers on CS ArXiv every month since 2020 are
                categorized under Learning or Artificial intelligence or
                Computer Vision.
              </p>
              <figure
                name="7066"
                id="7066"
                class="graf graf--figure graf-after--p"
              >
                <img
                  class="graf-image"
                  data-image-id="1*JQg2cSbgvO95rx9rxJ6CyA.png"
                  data-width="933"
                  data-height="453"
                  src="https://cdn-images-1.medium.com/max/800/1*JQg2cSbgvO95rx9rxJ6CyA.png"
                />
                <figcaption class="imageCaption">
                  Time series of papers published in AI, Computer Vision, and ML
                  on ArXiv. Figure derived from
                  <a
                    href="https://sci-genie.com/"
                    data-href="https://sci-genie.com/"
                    class="markup--anchor markup--figure-anchor"
                    rel="noopener noreferrer noopener"
                    target="_blank"
                    >https://sci-genie.com/</a
                  >
                </figcaption>
              </figure>
              <p name="4a5f" id="4a5f" class="graf graf--p graf-after--figure">
                Why do I bring all of this up? Well, the technology industry
                changes very fast. In the early 2000s, CDs and DVDs were still
                around but rarely people use them today as one can stream 4K. In
                the tech space, many times a few fundamental ideas dramatically
                changed the industry over a very short period of time because
                they create substantial productivity/capital gains. Linux,
                Microsoft, and Apple created OS’s in the early 80/90’s which
                liberalized computing. The internet and browsers built an entire
                economy that runs on top of a simple application. Apple built
                the iPhone which created a revolution in the mobile computing
                space.
              </p>
              <p name="a8bc" id="a8bc" class="graf graf--p graf-after--p">
                <strong class="markup--strong markup--p-strong"
                  ><em class="markup--em markup--p-em"
                    >All of these examples involved a game-changing technology
                    that out beat its predecessor’s by several folds and made
                    the consumer way more productive or entertained than they
                    earlier were. These technologies created the
                    interface/abstraction through which our productivity grew
                    several folds.</em
                  ></strong
                >
                Even in SWE, we have become more productive because we have
                fancy IDEs and other tools which make our lives so much easier.
                We conduct our work through interfaces designed to improve our
                productivity. This is where the connection of these new “Neural
                technologies” can have a meaningful consumer side impact.
              </p>
              <p
                name="d82f"
                id="d82f"
                class="graf graf--p graf--startsWithDoubleQuote graf-after--p"
              >
                “Neural Network Like” technologies have the capability to
                operate in data domains with extremely unstructured information
                like Text, images, audio, video. These technologies can already
                handle cross-channel
                <a
                  href="https://openai.com/blog/dall-e/"
                  data-href="https://openai.com/blog/dall-e/"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >information generation</a
                >(Text→images or vice versa). This capability to handle diverse
                information channels can in theory also incorporate information
                coming via “Gestures”/”Voice”/”A User’s Surroundings” as input.
                When we see Jarvis in Iron man, Tony is able to talk and
                interact with Jarvis because the
                <strong class="markup--strong markup--p-strong"
                  >“World is the Interface”</strong
                >.
              </p>
              <p name="7364" id="7364" class="graf graf--p graf-after--p">
                To reach a point where the world is an interface we need more
                than just devices that can “Augmented Reality”. We also need
                “enough intelligence” in the system to derive information from
                very high level “information channels” like speech commands or
                gestures or environmental context or conversations. Creating a
                commercial scale application that uses “neural technologies” in
                the space of AR and VR for creating interfaces that improve
                productivity would be a beautiful confluence and its creation
                would be parallel to the creation of Linux in the software
                space.
              </p>
              <p name="dcec" id="dcec" class="graf graf--p graf-after--p">
                These words may seem very grand but my core gist is that these
                technologies will help fuel the next possible wave in tech which
                is could be around augmented reality and “intelligent assistants
                that are coupled with the user’s reality”. All of this may sound
                as if I am talking out my A**, but twenty-five years ago we
                would never have imagined computers making music or writing
                computer code from English descriptions. The other reason I am
                optimistic of this direction is after seeing companies like
                <a
                  href="https://spatial.io/"
                  data-href="https://spatial.io/"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >Spatial</a
                >
                emerge. Correlating the growth of these “neural technologies”, I
                don’t deem it far-fetched to imagine that a “Jarvis-like”
                assistant may just be available in the
                <em class="markup--em markup--p-em">not so far</em>* future.
              </p>
              <p name="520e" id="520e" class="graf graf--p graf-after--p">
                Currently, we are stuck with our keyboard and mice, and styluses
                as the interfaces for communication with our software systems.
                But from the current patterns and trends I see in the new
                discoveries/inventions in research, The confluence of AR and
                Artificial “Intelligence” seems to be a direction that can have
                most impact on consumer tech. There is also
                <a
                  href="https://www.techradar.com/news/apple-glasses"
                  data-href="https://www.techradar.com/news/apple-glasses"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >speculation around Apple working on AR glasses</a
                >. If they end up building usable “Apple Glass with a Siri that
                can understand context and environments” then the
                technology/platform can end up being game-changing.
              </p>
              <h3 name="4982" id="4982" class="graf graf--h3 graf-after--p">
                Prediction 2: Endless Scaling Via Kubernetes For Huge
                Neural Networks
              </h3>
              <p name="c7a3" id="c7a3" class="graf graf--p graf-after--h3">
                GPT-3 proved that making NN’s bigger is perfectly fine.
                Obviously, we can add more “Prior signals” in the input
                information and design fancier outputs like Videos or 3D
                point-clouds, but the
                <a
                  href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"
                  data-href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener"
                  target="_blank"
                  >Bitter Lesson</a
                >
                keeps being the same :
              </p>
              <blockquote
                name="6e7d"
                id="6e7d"
                class="graf graf--blockquote graf-after--p"
              >
                The bitter lesson is based on the historical observations that
                1) AI researchers have often tried to build knowledge into their
                agents, 2) this always helps in the short term, and is
                personally satisfying to the researcher, but 3) in the long run
                it plateaus and even inhibits further progress, and 4)
                breakthrough progress eventually arrives by an opposing approach
                based on scaling computation by search and learning. The
                eventual success is tinged with bitterness, and often
                incompletely digested, because it is success over a favored,
                human-centric approach.
              </blockquote>
              <blockquote
                name="caad"
                id="caad"
                class="graf graf--blockquote graf-after--blockquote"
              >
                One thing that should be learned from the bitter lesson is the
                great power of general purpose methods, of methods that continue
                to scale with increased computation even as the available
                computation becomes very great. The two methods that seem to
                scale arbitrarily in this way are
                <em class="markup--em markup--blockquote-em">search</em> and
                <em class="markup--em markup--blockquote-em">learning</em>
              </blockquote>
              <blockquote
                name="dfcd"
                id="dfcd"
                class="graf graf--blockquote graf-after--blockquote"
              >
                <em class="markup--em markup--blockquote-em"
                  >- Richard Sutton (Godfather of RL)</em
                >
              </blockquote>
              <figure
                name="9a95"
                id="9a95"
                class="graf graf--figure graf-after--blockquote"
              >
                <img
                  class="graf-image"
                  data-image-id="1*esmyqoCh7Q-HnqVxpHa0CQ.png"
                  data-width="947"
                  data-height="946"
                  src="https://cdn-images-1.medium.com/max/800/1*esmyqoCh7Q-HnqVxpHa0CQ.png"
                />
                <figcaption class="imageCaption">
                  Neural Network Go BRRRR!.
                </figcaption>
              </figure>
              <p name="b74c" id="b74c" class="graf graf--p graf-after--figure">
                If research keeps carrying on in this direction to build large
                “General” purpose models that can work for arbitrary types of
                data then, there would be a paramount need for extremely
                scalable manageable “service-orchestration”. OpenAI scaled their
                Kubernetes cluster
                <a
                  href="https://openai.com/blog/scaling-kubernetes-to-7500-nodes/"
                  data-href="https://openai.com/blog/scaling-kubernetes-to-7500-nodes/"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener noreferrer noopener"
                  target="_blank"
                  >to 7500 nodes. Wow.</a
                >
                This is a feat in itself. If the technology trend towards
                leveraging Kubernetes for service orchestration continues then
                there are rich avenues for building “open-source abstractions”
                which can efficiently help manage and orchestrate such large
                clusters for training huge neural networks. There is tech like
                <a
                  href="https://www.kubeflow.org/"
                  data-href="https://www.kubeflow.org/"
                  class="markup--anchor markup--p-anchor"
                  rel="noopener noreferrer noopener"
                  target="_blank"
                  >Kubeflow</a
                >
                but working with a complex distributed system(Large model
                training) at such a huge scale requires much more complicated
                tools which are not yet available off-the-shelf. Over the next
                few years, there will be emergent open-source-tools that will
                make this process much easier and simpler.
              </p>
              <h3 name="ee8a" id="ee8a" class="graf graf--h3 graf-after--p">
                Conclusions
              </h3>
              <p name="8445" id="8445" class="graf graf--p graf-after--h3">
                Everything that I have written about in this blog post, should
                be taken with a grain of salt. I have worked quite a lot with
                this technology in the past year to firmly say that many times
                these approaches are just brittle!. I would always recommend
                starting with heuristics before jumping to ML. Many times the
                heuristic can take you a long way before having the need to
                throw ML at a problem. But the optimistic side is that potential
                applications and avenues where these will be used are going to
                grow because of accessibility to compute and large populations
                of people focusing on these problems. And maybe who knows,
                Jarvis may actually be right around the corner. 
              </p>
              <p
                name="d04c"
                id="d04c"
                class="graf graf--p graf--empty graf-after--p graf--trailing"
              >
                <br />
              </p>
            </div>
          </div>
        </section>
      </section>
      <footer>
        <p><a href="https://medium.com/p/8b4eb605f2f8">View original.</a></p>
        <p>
          Exported from <a href="https://medium.com">Medium</a> on April 8,
          2021.
        </p>
      </footer>
    </article>
  </body>
</html>
